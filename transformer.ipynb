{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shadabamd\\AppData\\Local\\anaconda3\\envs\\semantic\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      ".gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 116kB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<?, ?B/s] \n",
      "README.md: 100%|██████████| 10.6k/10.6k [00:00<?, ?B/s]\n",
      "config.json: 100%|██████████| 612/612 [00:00<?, ?B/s] \n",
      "config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 7.47kB/s]\n",
      "data_config.json: 100%|██████████| 39.3k/39.3k [00:00<?, ?B/s]\n",
      "pytorch_model.bin: 100%|██████████| 90.9M/90.9M [00:06<00:00, 15.0MB/s]\n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<?, ?B/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 7.24kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 744kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 350/350 [00:00<?, ?B/s] \n",
      "train_script.py: 100%|██████████| 13.2k/13.2k [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.08MB/s]\n",
      "modules.json: 100%|██████████| 349/349 [00:00<?, ?B/s] \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shadabamd\\AppData\\Local\\anaconda3\\envs\\semantic\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage (Sentence-Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 768)\n",
      "[[ 0.02250257 -0.07829178 -0.02303074 ... -0.00827928  0.02652692\n",
      "  -0.00201897]\n",
      " [ 0.04170237  0.00109738 -0.01553418 ... -0.02181629 -0.06359358\n",
      "  -0.00875285]]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage (HuggingFace Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 363/363 [00:00<?, ?B/s] \n",
      "c:\\Users\\shadabamd\\AppData\\Local\\anaconda3\\envs\\semantic\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shadabamd\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 510kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 812kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 239/239 [00:00<?, ?B/s] \n",
      "config.json: 100%|██████████| 571/571 [00:00<00:00, 66.2kB/s]\n",
      "pytorch_model.bin: 100%|██████████| 438M/438M [00:31<00:00, 14.1MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'this', 'is', 'an', 'example', 'sentence', '</s>', '</s>', 'each', 'sentence', 'is', 'converted', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# print(tokenizer.tokenize(sentences, add_special_tokens=True, padding=False))\n",
    "print(tokenizer.tokenize(sentences, add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "{'input_ids': tensor([[   0, 2027, 2007, 2023, 2746, 6255,    2],\n",
      "        [   0, 2173, 6255, 2007, 4995,    2,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]])}\n",
      "tensor([[   0, 2027, 2007, 2023, 2746, 6255,    2],\n",
      "        [   0, 2173, 6255, 2007, 4995,    2,    1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]])\n",
      "no. of tokens in a sentence =  7\n"
     ]
    }
   ],
   "source": [
    "print(type(encoded_input))\n",
    "print(encoded_input)\n",
    "print(encoded_input[\"input_ids\"])\n",
    "print(encoded_input[\"attention_mask\"])\n",
    "print(\"no. of tokens in a sentence = \", encoded_input[\"input_ids\"].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
      "2\n",
      "last_hidden_state ==> torch.Size([2, 7, 768]) torch.Size([2, 7, 768])\n",
      "pooler_output ==> torch.Size([2, 768]) torch.Size([2, 768])\n",
      "tensor([[ 0.0880, -0.0418,  0.0182,  ...,  0.0867, -0.0284, -0.0387],\n",
      "        [-0.0302, -0.0482, -0.0346,  ...,  0.0619, -0.0250,  0.0269]])\n"
     ]
    }
   ],
   "source": [
    "print(type(model_output))\n",
    "print(len(model_output))\n",
    "print(f\"last_hidden_state\", \"==>\",model_output[0].shape, model_output.last_hidden_state.shape)\n",
    "print(f\"pooler_output\", \"==>\", model_output[1].shape, model_output.pooler_output.shape)\n",
    "print(model_output.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0616, -0.2143, -0.0630,  ..., -0.0227,  0.0726, -0.0055],\n",
      "        [ 0.1287,  0.0034, -0.0479,  ..., -0.0673, -0.1962, -0.0270]])\n",
      "tensor([[ 0.0225, -0.0783, -0.0230,  ..., -0.0083,  0.0265, -0.0020],\n",
      "        [ 0.0417,  0.0011, -0.0155,  ..., -0.0218, -0.0636, -0.0088]])\n"
     ]
    }
   ],
   "source": [
    "print(sentence_embeddings)\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 570/570 [00:00<00:00, 211kB/s]\n",
      "c:\\Users\\shadabamd\\AppData\\Local\\anaconda3\\envs\\semantic\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shadabamd\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|██████████| 440M/440M [00:53<00:00, 8.31MB/s] \n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'She is a MachineLearning Engineer and works in California'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 3.73kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 530kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 670kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she', 'is', 'a', 'machine', '##lea', '##rn', '##ing', 'engineer', 'and', 'works', 'in', 'california']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['[CLS]'] + tokens + ['[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'she', 'is', 'a', 'machine', '##lea', '##rn', '##ing', 'engineer', 'and', 'works', 'in', 'california', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens + ['[PAD]'] + ['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'she', 'is', 'a', 'machine', '##lea', '##rn', '##ing', 'engineer', 'and', 'works', 'in', 'california', '[SEP]', '[PAD]', '[PAD]']\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2016, 2003, 1037, 3698, 19738, 6826, 2075, 3992, 1998, 2573, 1999, 2662, 102, 0, 0]\n",
      "['[CLS]', 'she', 'is', 'a', 'machine', '##lea', '##rn', '##ing', 'engineer', 'and', 'works', 'in', 'california', '[SEP]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "#unique token ID\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "attention_mask = torch.tensor(attention_mask).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>\n",
      "2\n",
      "last_hidden_state ==> torch.Size([1, 16, 768]) torch.Size([1, 16, 768])\n",
      "tensor([[[-0.1925,  0.1684, -0.4252,  ..., -0.2599,  0.3736,  0.0529],\n",
      "         [ 0.2417, -0.2748, -0.4909,  ...,  0.1372,  0.3408, -0.4655],\n",
      "         [-0.0871,  0.0837,  0.2605,  ..., -0.4635, -0.0462,  0.2621],\n",
      "         ...,\n",
      "         [ 0.6711, -0.0076, -0.3847,  ..., -0.1289, -0.5171, -0.8002],\n",
      "         [-0.2731,  0.1098, -0.5440,  ...,  0.0314,  0.4467, -0.3448],\n",
      "         [-0.2387,  0.0119, -0.4760,  ...,  0.4656,  0.5837, -0.3774]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "pooler_output ==> torch.Size([1, 768]) torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Getting the embedding\n",
    "output = model(token_ids, attention_mask = attention_mask)\n",
    "\n",
    "print(type(output))\n",
    "print(len(output))\n",
    "print(f\"last_hidden_state\", \"==>\",output[0].shape, output.last_hidden_state.shape)\n",
    "print(output.last_hidden_state)\n",
    "print(f\"pooler_output\", \"==>\", output[1].shape, output.pooler_output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shadabamd\\AppData\\Local\\anaconda3\\envs\\semantic\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      ".gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 116kB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<?, ?B/s] \n",
      "README.md: 100%|██████████| 10.6k/10.6k [00:00<?, ?B/s]\n",
      "config.json: 100%|██████████| 612/612 [00:00<?, ?B/s] \n",
      "config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 7.47kB/s]\n",
      "data_config.json: 100%|██████████| 39.3k/39.3k [00:00<?, ?B/s]\n",
      "pytorch_model.bin: 100%|██████████| 90.9M/90.9M [00:06<00:00, 15.0MB/s]\n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<?, ?B/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 7.24kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 744kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 350/350 [00:00<?, ?B/s] \n",
      "train_script.py: 100%|██████████| 13.2k/13.2k [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.08MB/s]\n",
      "modules.json: 100%|██████████| 349/349 [00:00<?, ?B/s] \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shadabamd\\AppData\\Local\\anaconda3\\envs\\semantic\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage (Sentence-Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 768)\n",
      "[[ 0.02250257 -0.07829178 -0.02303074 ... -0.00827928  0.02652692\n",
      "  -0.00201897]\n",
      " [ 0.04170237  0.00109738 -0.01553418 ... -0.02181629 -0.06359358\n",
      "  -0.00875285]]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage (HuggingFace Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 363/363 [00:00<?, ?B/s] \n",
      "c:\\Users\\shadabamd\\AppData\\Local\\anaconda3\\envs\\semantic\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shadabamd\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 510kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 812kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 239/239 [00:00<?, ?B/s] \n",
      "config.json: 100%|██████████| 571/571 [00:00<00:00, 66.2kB/s]\n",
      "pytorch_model.bin: 100%|██████████| 438M/438M [00:31<00:00, 14.1MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'this', 'is', 'an', 'example', 'sentence', '</s>', '</s>', 'each', 'sentence', 'is', 'converted', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# print(tokenizer.tokenize(sentences, add_special_tokens=True, padding=False))\n",
    "print(tokenizer.tokenize(sentences, add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "{'input_ids': tensor([[   0, 2027, 2007, 2023, 2746, 6255,    2],\n",
      "        [   0, 2173, 6255, 2007, 4995,    2,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]])}\n",
      "tensor([[   0, 2027, 2007, 2023, 2746, 6255,    2],\n",
      "        [   0, 2173, 6255, 2007, 4995,    2,    1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]])\n",
      "no. of tokens in a sentence =  7\n"
     ]
    }
   ],
   "source": [
    "print(type(encoded_input))\n",
    "print(encoded_input)\n",
    "print(encoded_input[\"input_ids\"])\n",
    "print(encoded_input[\"attention_mask\"])\n",
    "print(\"no. of tokens in a sentence = \", encoded_input[\"input_ids\"].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
      "2\n",
      "last_hidden_state ==> torch.Size([2, 7, 768]) torch.Size([2, 7, 768])\n",
      "pooler_output ==> torch.Size([2, 768]) torch.Size([2, 768])\n",
      "tensor([[ 0.0880, -0.0418,  0.0182,  ...,  0.0867, -0.0284, -0.0387],\n",
      "        [-0.0302, -0.0482, -0.0346,  ...,  0.0619, -0.0250,  0.0269]])\n"
     ]
    }
   ],
   "source": [
    "print(type(model_output))\n",
    "print(len(model_output))\n",
    "print(f\"last_hidden_state\", \"==>\",model_output[0].shape, model_output.last_hidden_state.shape)\n",
    "print(f\"pooler_output\", \"==>\", model_output[1].shape, model_output.pooler_output.shape)\n",
    "print(model_output.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0616, -0.2143, -0.0630,  ..., -0.0227,  0.0726, -0.0055],\n",
      "        [ 0.1287,  0.0034, -0.0479,  ..., -0.0673, -0.1962, -0.0270]])\n",
      "tensor([[ 0.0225, -0.0783, -0.0230,  ..., -0.0083,  0.0265, -0.0020],\n",
      "        [ 0.0417,  0.0011, -0.0155,  ..., -0.0218, -0.0636, -0.0088]])\n"
     ]
    }
   ],
   "source": [
    "print(sentence_embeddings)\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 570/570 [00:00<00:00, 211kB/s]\n",
      "c:\\Users\\shadabamd\\AppData\\Local\\anaconda3\\envs\\semantic\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shadabamd\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|██████████| 440M/440M [00:53<00:00, 8.31MB/s] \n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'She is a MachineLearning Engineer and works in California'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 3.73kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 530kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 670kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she', 'is', 'a', 'machine', '##lea', '##rn', '##ing', 'engineer', 'and', 'works', 'in', 'california']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['[CLS]'] + tokens + ['[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'she', 'is', 'a', 'machine', '##lea', '##rn', '##ing', 'engineer', 'and', 'works', 'in', 'california', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens + ['[PAD]'] + ['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'she', 'is', 'a', 'machine', '##lea', '##rn', '##ing', 'engineer', 'and', 'works', 'in', 'california', '[SEP]', '[PAD]', '[PAD]']\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2016, 2003, 1037, 3698, 19738, 6826, 2075, 3992, 1998, 2573, 1999, 2662, 102, 0, 0]\n",
      "['[CLS]', 'she', 'is', 'a', 'machine', '##lea', '##rn', '##ing', 'engineer', 'and', 'works', 'in', 'california', '[SEP]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "#unique token ID\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "attention_mask = torch.tensor(attention_mask).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>\n",
      "2\n",
      "last_hidden_state ==> torch.Size([1, 16, 768]) torch.Size([1, 16, 768])\n",
      "tensor([[[-0.1925,  0.1684, -0.4252,  ..., -0.2599,  0.3736,  0.0529],\n",
      "         [ 0.2417, -0.2748, -0.4909,  ...,  0.1372,  0.3408, -0.4655],\n",
      "         [-0.0871,  0.0837,  0.2605,  ..., -0.4635, -0.0462,  0.2621],\n",
      "         ...,\n",
      "         [ 0.6711, -0.0076, -0.3847,  ..., -0.1289, -0.5171, -0.8002],\n",
      "         [-0.2731,  0.1098, -0.5440,  ...,  0.0314,  0.4467, -0.3448],\n",
      "         [-0.2387,  0.0119, -0.4760,  ...,  0.4656,  0.5837, -0.3774]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "pooler_output ==> torch.Size([1, 768]) torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Getting the embedding\n",
    "output = model(token_ids, attention_mask = attention_mask)\n",
    "\n",
    "print(type(output))\n",
    "print(len(output))\n",
    "print(f\"last_hidden_state\", \"==>\",output[0].shape, output.last_hidden_state.shape)\n",
    "print(output.last_hidden_state)\n",
    "print(f\"pooler_output\", \"==>\", output[1].shape, output.pooler_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiohttp==3.9.1\n",
      "aiosignal==1.3.1\n",
      "annotated-types==0.6.0\n",
      "anyio==3.7.1\n",
      "asttokens==2.4.1\n",
      "async-timeout==4.0.3\n",
      "attrs==23.1.0\n",
      "backcall==0.2.0\n",
      "backoff==2.2.1\n",
      "backports.functools-lru-cache==1.6.5\n",
      "bcrypt==4.1.2\n",
      "beautifulsoup4==4.12.2\n",
      "cachetools==5.3.2\n",
      "certifi==2023.11.17\n",
      "chardet==5.2.0\n",
      "charset-normalizer==3.3.2\n",
      "chroma-hnswlib==0.7.3\n",
      "chromadb==0.4.17\n",
      "click==8.1.7\n",
      "colorama==0.4.6\n",
      "coloredlogs==15.0.1\n",
      "comm==0.2.0\n",
      "dataclasses-json==0.6.3\n",
      "debugpy==1.8.0\n",
      "decorator==5.1.1\n",
      "Deprecated==1.2.14\n",
      "distro==1.9.0\n",
      "dnspython==2.4.2\n",
      "emoji==2.9.0\n",
      "exceptiongroup==1.2.0\n",
      "executing==2.0.1\n",
      "fastapi==0.105.0\n",
      "filelock==3.13.1\n",
      "filetype==1.2.0\n",
      "flatbuffers==23.5.26\n",
      "frozenlist==1.4.1\n",
      "fsspec==2023.12.2\n",
      "google-auth==2.25.2\n",
      "google-search-results==2.4.2\n",
      "googleapis-common-protos==1.62.0\n",
      "graphlib-backport==1.0.3\n",
      "greenlet==3.0.3\n",
      "grpcio==1.60.0\n",
      "h11==0.14.0\n",
      "httpcore==1.0.2\n",
      "httptools==0.6.1\n",
      "httpx==0.26.0\n",
      "huggingface-hub==0.20.1\n",
      "humanfriendly==10.0\n",
      "idna==3.6\n",
      "importlib-metadata==6.11.0\n",
      "importlib-resources==6.1.1\n",
      "ipykernel==6.26.0\n",
      "ipython==8.12.3\n",
      "jedi==0.19.1\n",
      "Jinja2==3.1.2\n",
      "joblib==1.3.2\n",
      "jsonpatch==1.33\n",
      "jsonpointer==2.4\n",
      "jupyter_client==8.6.0\n",
      "jupyter_core==5.5.1\n",
      "kubernetes==28.1.0\n",
      "langchain==0.0.334\n",
      "langdetect==1.0.9\n",
      "langsmith==0.0.75\n",
      "loguru==0.7.2\n",
      "lxml==4.9.4\n",
      "MarkupSafe==2.1.3\n",
      "marshmallow==3.20.1\n",
      "matplotlib-inline==0.1.6\n",
      "monotonic==1.6\n",
      "mpmath==1.3.0\n",
      "multidict==6.0.4\n",
      "mypy-extensions==1.0.0\n",
      "nest-asyncio==1.5.8\n",
      "networkx==3.1\n",
      "nltk==3.8.1\n",
      "numexpr==2.8.6\n",
      "numpy==1.24.4\n",
      "oauthlib==3.2.2\n",
      "onnxruntime==1.16.3\n",
      "openai==1.2.3\n",
      "opentelemetry-api==1.22.0\n",
      "opentelemetry-exporter-otlp-proto-common==1.22.0\n",
      "opentelemetry-exporter-otlp-proto-grpc==1.22.0\n",
      "opentelemetry-proto==1.22.0\n",
      "opentelemetry-sdk==1.22.0\n",
      "opentelemetry-semantic-conventions==0.43b0\n",
      "overrides==7.4.0\n",
      "packaging==23.2\n",
      "pandas==2.0.3\n",
      "parso==0.8.3\n",
      "pickleshare==0.7.5\n",
      "Pillow==10.1.0\n",
      "pinecone-client==2.2.4\n",
      "pipdeptree==2.13.1\n",
      "platformdirs==4.1.0\n",
      "posthog==3.1.0\n",
      "prompt-toolkit==3.0.43\n",
      "protobuf==4.25.1\n",
      "psutil==5.9.7\n",
      "pulsar-client==3.3.0\n",
      "pure-eval==0.2.2\n",
      "pyasn1==0.5.1\n",
      "pyasn1-modules==0.3.0\n",
      "pydantic==2.5.3\n",
      "pydantic_core==2.14.6\n",
      "Pygments==2.17.2\n",
      "PyPika==0.48.9\n",
      "pyreadline3==3.4.1\n",
      "python-dateutil==2.8.2\n",
      "python-dotenv==1.0.0\n",
      "python-iso639==2023.12.11\n",
      "python-magic==0.4.27\n",
      "pytz==2023.3.post1\n",
      "pywin32==306\n",
      "PyYAML==6.0.1\n",
      "pyzmq==25.1.2\n",
      "rapidfuzz==3.5.2\n",
      "regex==2023.12.25\n",
      "requests==2.31.0\n",
      "requests-oauthlib==1.3.1\n",
      "rsa==4.9\n",
      "safetensors==0.4.1\n",
      "scikit-learn==1.3.2\n",
      "scipy==1.10.1\n",
      "sentence-transformers==2.2.2\n",
      "sentencepiece==0.1.99\n",
      "six==1.16.0\n",
      "sniffio==1.3.0\n",
      "soupsieve==2.5\n",
      "SQLAlchemy==2.0.23\n",
      "stack-data==0.6.3\n",
      "starlette==0.27.0\n",
      "sympy==1.12\n",
      "tabulate==0.9.0\n",
      "tenacity==8.2.3\n",
      "threadpoolctl==3.2.0\n",
      "tiktoken==0.5.1\n",
      "tokenizers==0.15.0\n",
      "torch==2.1.2\n",
      "torchvision==0.16.2\n",
      "tornado==6.4\n",
      "tqdm==4.66.1\n",
      "traitlets==5.14.0\n",
      "transformers==4.36.2\n",
      "typer==0.9.0\n",
      "typing-inspect==0.9.0\n",
      "typing_extensions==4.9.0\n",
      "tzdata==2023.3\n",
      "unstructured==0.10.30\n",
      "urllib3==1.26.18\n",
      "uvicorn==0.25.0\n",
      "watchfiles==0.21.0\n",
      "wcwidth==0.2.12\n",
      "websocket-client==1.7.0\n",
      "websockets==12.0\n",
      "win32-setctime==1.1.0\n",
      "wrapt==1.16.0\n",
      "yarl==1.9.4\n",
      "zipp==3.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
